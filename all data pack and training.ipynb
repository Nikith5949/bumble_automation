{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f43d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import signal\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c275d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_resize(image,desired_width = 244,desired_height = 244):\n",
    "    current_height, current_width = image.shape\n",
    "    aspect_ratio = current_width / current_height\n",
    "\n",
    "    if aspect_ratio > 1:\n",
    "        # Landscape image\n",
    "        new_width = desired_width\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        # Portrait or square image\n",
    "        new_height = desired_height\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    # Add padding to match desired width and height\n",
    "    top_pad = (desired_height - new_height) // 2\n",
    "    bottom_pad = desired_height - new_height - top_pad\n",
    "    left_pad = (desired_width - new_width) // 2\n",
    "    right_pad = desired_width - new_width - left_pad\n",
    "\n",
    "    image = cv2.copyMakeBorder(image, top_pad, bottom_pad, left_pad, right_pad, cv2.BORDER_CONSTANT, value=0)\n",
    "\n",
    "    return image\n",
    "\n",
    "def training_existing_data():\n",
    "    dataset_folder = \"source_pre\"\n",
    "    images=[]\n",
    "    ratings=[]\n",
    "    for filename in os.listdir(dataset_folder):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            rating= int(filename.split('_')[1][0])\n",
    "#             print(rating)\n",
    "\n",
    "            image = cv2.imread(dataset_folder + '/' + filename)\n",
    "            # print(image)\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            image= img_resize(gray_image)\n",
    "            if rating >=1 and rating<=5:\n",
    "                images.append(image)\n",
    "                ratings.append(rating)\n",
    "#             cv2.imshow('image', image)\n",
    "            # time.sleep(10)\n",
    "#             cv2.waitKey(25)\n",
    "    images = np.array(images)\n",
    "    ratings = np.array(ratings)\n",
    "#     print(\"Images shape:\", images.shape)\n",
    "#     print(\"Ratings shape:\", ratings.shape)\n",
    "    return images,ratings\n",
    "\n",
    "images,ratings=training_existing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a5d7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(images, ratings, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02feec4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((534, 244, 244), (534,), (134, 244, 244), (134,))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape,X_val.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7530e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6006a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f3e8e9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, 244, 1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_val = np.expand_dims(X_val, axis=-1)\n",
    "X_val[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "61b9f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_categorical = tf.keras.utils.to_categorical(y_train - 1, num_classes=5)\n",
    "y_val_categorical = tf.keras.utils.to_categorical(y_val - 1, num_classes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a2bdd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data type to int32\n",
    "y_train_categorical = y_train_categorical.astype(np.int32)\n",
    "y_val_categorical = y_val_categorical.astype(np.int32)\n",
    "y_train_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd64a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(-1, 244, 244, 1)\n",
    "# X_val = X_val.reshape(-1, 244, 244, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3825c6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((534, 244, 244, 1), (534, 5), (134, 244, 244, 1), (134, 5))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train_categorical.shape,X_val.shape,y_val_categorical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bab000",
   "metadata": {},
   "source": [
    "# model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26b143a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b6ef83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_21 (Conv2D)          (None, 242, 242, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 121, 121, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 119, 119, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 59, 59, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 57, 57, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 28, 28, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                6422592   \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,515,589\n",
      "Trainable params: 6,515,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(244, 244, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(5, activation='softmax')  # Use softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.009)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33ab4179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 14s 815ms/step - loss: 1.3964 - accuracy: 0.3109 - val_loss: 1.3869 - val_accuracy: 0.3881\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 14s 817ms/step - loss: 1.3903 - accuracy: 0.3539 - val_loss: 1.3506 - val_accuracy: 0.3134\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.3834 - accuracy: 0.3352 - val_loss: 1.3636 - val_accuracy: 0.3881\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 14s 795ms/step - loss: 1.3828 - accuracy: 0.3221 - val_loss: 1.3601 - val_accuracy: 0.3881\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 14s 816ms/step - loss: 1.3790 - accuracy: 0.3539 - val_loss: 1.3549 - val_accuracy: 0.3881\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 14s 826ms/step - loss: 1.3871 - accuracy: 0.3539 - val_loss: 1.3556 - val_accuracy: 0.3881\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 14s 801ms/step - loss: 1.3794 - accuracy: 0.3539 - val_loss: 1.3931 - val_accuracy: 0.3881\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 14s 796ms/step - loss: 1.3919 - accuracy: 0.3539 - val_loss: 1.3561 - val_accuracy: 0.3881\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 14s 819ms/step - loss: 1.3878 - accuracy: 0.3539 - val_loss: 1.3669 - val_accuracy: 0.3881\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 14s 824ms/step - loss: 1.3839 - accuracy: 0.3277 - val_loss: 1.3493 - val_accuracy: 0.3881\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 14s 802ms/step - loss: 1.3975 - accuracy: 0.3539 - val_loss: 1.3776 - val_accuracy: 0.3881\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 14s 789ms/step - loss: 1.3858 - accuracy: 0.3333 - val_loss: 1.3722 - val_accuracy: 0.3134\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 14s 833ms/step - loss: 1.3867 - accuracy: 0.3240 - val_loss: 1.3683 - val_accuracy: 0.3134\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 14s 811ms/step - loss: 1.3922 - accuracy: 0.2996 - val_loss: 1.3732 - val_accuracy: 0.3881\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 1.3816 - accuracy: 0.3539 - val_loss: 1.3585 - val_accuracy: 0.3881\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.3750 - accuracy: 0.3539 - val_loss: 1.3545 - val_accuracy: 0.3881\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 14s 813ms/step - loss: 1.3739 - accuracy: 0.3539 - val_loss: 1.3524 - val_accuracy: 0.3881\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 14s 824ms/step - loss: 1.3730 - accuracy: 0.3539 - val_loss: 1.3509 - val_accuracy: 0.3881\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 14s 815ms/step - loss: 1.3728 - accuracy: 0.3539 - val_loss: 1.3508 - val_accuracy: 0.3881\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 1.3731 - accuracy: 0.3539 - val_loss: 1.3527 - val_accuracy: 0.3881\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_categorical, validation_data=(X_val, y_val_categorical), epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4bdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bumb",
   "language": "python",
   "name": "bumb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
